{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Customized Estimator for Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tabnet.utils.logger import init_logger\n",
    "from tabnet.estimator import CustomizedEstimator\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_dir = 'logs'\n",
    "logger_name = 'Customized Estimator DEMO'\n",
    "level = 'INFO'\n",
    "\n",
    "logger = init_logger(logger_dir=logger_dir, logger_name=logger_name, level=level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "### Init a `CustomizedEstimator` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = CustomizedEstimator(\n",
    "    input_dims=30, output_dims=[1], logger=logger, is_cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "### Defined your customized `loss function` and `post-processor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Import base classes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnet.base import Loss, BaseTabNet, BasePostProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Define customized loss function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLoss(Loss):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "        self._loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def score_func(self, preds, targets):\n",
    "        targets = targets.to(preds.device)\n",
    "        return self._loss_fn(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Define customized post-processor***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPostProcessor(BasePostProcessor):\n",
    "    def __init__(self, is_cuda):            \n",
    "        super(MyPostProcessor, self).__init__(\n",
    "            num_tasks=1, is_cuda=is_cuda, output_dims=[1]\n",
    "        )\n",
    "        \n",
    "    def _build(self, num_tasks, output_dims):\n",
    "        self._processors.append(torch.nn.Sigmoid())\n",
    "        return \n",
    "    \n",
    "    def forward(self, x, is_return_proba=False):\n",
    "        proba = self._processors[0](x[0])\n",
    "        label = (proba > 0.5) * 1\n",
    "        \n",
    "        if is_return_proba:\n",
    "            return [label], [proba]\n",
    "        else:\n",
    "            return [label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "### Register customized `objects`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomizedEstimator(input_dims=30, output_dims=[1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.register_loss(MyLoss()).register_postprocessor(MyPostProcessor(is_cuda=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4\n",
    "### Train your own model !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Load data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Build network architecture***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-23 21:48:37,348][WARNING][TabNet] Failed to load model from None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomizedEstimator(input_dims=30, output_dims=[1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.build(path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-23 21:48:37,368][INFO][TabNet] Show model architecture.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InferenceModel(\n",
      "  (embedding_encoder): EmbeddingEncoder()\n",
      "  (tabnet_encoder): TabNetEncoder(\n",
      "    (input_bn): BatchNorm1d(30, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "    (input_splitter): FeatureTransformer(\n",
      "      (shared_block): FeatureBlock(\n",
      "        (shared_layers): ModuleList(\n",
      "          (0): Linear(in_features=30, out_features=32, bias=False)\n",
      "          (1): Linear(in_features=16, out_features=32, bias=False)\n",
      "        )\n",
      "        (glu_blocks): ModuleList(\n",
      "          (0): GLUBlock(\n",
      "            (fc): Linear(in_features=30, out_features=32, bias=False)\n",
      "            (gbn): GhostBatchNorm(\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): GLUBlock(\n",
      "            (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "            (gbn): GhostBatchNorm(\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (indep_block): FeatureBlock(\n",
      "        (glu_blocks): ModuleList(\n",
      "          (0): GLUBlock(\n",
      "            (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "            (gbn): GhostBatchNorm(\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): GLUBlock(\n",
      "            (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "            (gbn): GhostBatchNorm(\n",
      "              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (feats_transformers): ModuleList(\n",
      "      (0): FeatureTransformer(\n",
      "        (shared_block): FeatureBlock(\n",
      "          (shared_layers): ModuleList(\n",
      "            (0): Linear(in_features=30, out_features=32, bias=False)\n",
      "            (1): Linear(in_features=16, out_features=32, bias=False)\n",
      "          )\n",
      "          (glu_blocks): ModuleList(\n",
      "            (0): GLUBlock(\n",
      "              (fc): Linear(in_features=30, out_features=32, bias=False)\n",
      "              (gbn): GhostBatchNorm(\n",
      "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): GLUBlock(\n",
      "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "              (gbn): GhostBatchNorm(\n",
      "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (indep_block): FeatureBlock(\n",
      "          (glu_blocks): ModuleList(\n",
      "            (0): GLUBlock(\n",
      "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "              (gbn): GhostBatchNorm(\n",
      "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): GLUBlock(\n",
      "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "              (gbn): GhostBatchNorm(\n",
      "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): FeatureTransformer(\n",
      "        (shared_block): FeatureBlock(\n",
      "          (shared_layers): ModuleList(\n",
      "            (0): Linear(in_features=30, out_features=32, bias=False)\n",
      "            (1): Linear(in_features=16, out_features=32, bias=False)\n",
      "          )\n",
      "          (glu_blocks): ModuleList(\n",
      "            (0): GLUBlock(\n",
      "              (fc): Linear(in_features=30, out_features=32, bias=False)\n",
      "              (gbn): GhostBatchNorm(\n",
      "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): GLUBlock(\n",
      "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "              (gbn): GhostBatchNorm(\n",
      "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (indep_block): FeatureBlock(\n",
      "          (glu_blocks): ModuleList(\n",
      "            (0): GLUBlock(\n",
      "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "              (gbn): GhostBatchNorm(\n",
      "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): GLUBlock(\n",
      "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "              (gbn): GhostBatchNorm(\n",
      "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): FeatureTransformer(\n",
      "        (shared_block): FeatureBlock(\n",
      "          (shared_layers): ModuleList(\n",
      "            (0): Linear(in_features=30, out_features=32, bias=False)\n",
      "            (1): Linear(in_features=16, out_features=32, bias=False)\n",
      "          )\n",
      "          (glu_blocks): ModuleList(\n",
      "            (0): GLUBlock(\n",
      "              (fc): Linear(in_features=30, out_features=32, bias=False)\n",
      "              (gbn): GhostBatchNorm(\n",
      "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): GLUBlock(\n",
      "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "              (gbn): GhostBatchNorm(\n",
      "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (indep_block): FeatureBlock(\n",
      "          (glu_blocks): ModuleList(\n",
      "            (0): GLUBlock(\n",
      "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "              (gbn): GhostBatchNorm(\n",
      "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): GLUBlock(\n",
      "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
      "              (gbn): GhostBatchNorm(\n",
      "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (atten_transformers): ModuleList(\n",
      "      (0): AttentiveTransformer(\n",
      "        (fc): Linear(in_features=8, out_features=30, bias=False)\n",
      "        (gbn): GhostBatchNorm(\n",
      "          (bn): BatchNorm1d(30, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (selector): Sparsemax()\n",
      "      )\n",
      "      (1): AttentiveTransformer(\n",
      "        (fc): Linear(in_features=8, out_features=30, bias=False)\n",
      "        (gbn): GhostBatchNorm(\n",
      "          (bn): BatchNorm1d(30, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (selector): Sparsemax()\n",
      "      )\n",
      "      (2): AttentiveTransformer(\n",
      "        (fc): Linear(in_features=8, out_features=30, bias=False)\n",
      "        (gbn): GhostBatchNorm(\n",
      "          (bn): BatchNorm1d(30, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (selector): Sparsemax()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (tabnet_head): TabNetHead(\n",
      "    (heads): ModuleList(\n",
      "      (0): Linear(in_features=8, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_model.show_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Start training !!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "training_params = {\n",
    "    'batch_size': 512,\n",
    "    'max_epochs': 100,\n",
    "    'optimizer': Adam,\n",
    "    'optimizer_params': {'lr': 0.1},\n",
    "    'schedulers': [lr_scheduler.ExponentialLR],\n",
    "    'scheduler_params': {'gamma': 0.99}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-23 21:48:37,401][INFO][TabNet] Convert to inference model.\n",
      "[2021-02-23 21:48:37,403][INFO][TabNet] start training.\n",
      "[2021-02-23 21:48:37,404][INFO][TabNet] ******************** epoch : 0 ********************\n",
      "[2021-02-23 21:48:42,238][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:48:42,239][INFO][TabNet] total_loss : 0.8457855582237244\n",
      "[2021-02-23 21:48:42,239][INFO][TabNet] task_loss : 0.844632625579834\n",
      "[2021-02-23 21:48:42,240][INFO][TabNet] mask_loss : -1.152915596961975\n",
      "[2021-02-23 21:48:42,241][INFO][TabNet] time_cost : 0.945627\n",
      "[2021-02-23 21:48:42,241][INFO][TabNet] ******************** epoch : 1 ********************\n",
      "[2021-02-23 21:48:46,175][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:48:46,176][INFO][TabNet] total_loss : 0.62274169921875\n",
      "[2021-02-23 21:48:46,177][INFO][TabNet] task_loss : 0.6216962337493896\n",
      "[2021-02-23 21:48:46,178][INFO][TabNet] mask_loss : -1.045465111732483\n",
      "[2021-02-23 21:48:46,178][INFO][TabNet] time_cost : 0.0607\n",
      "[2021-02-23 21:48:46,179][INFO][TabNet] ******************** epoch : 2 ********************\n",
      "[2021-02-23 21:48:50,122][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:48:50,122][INFO][TabNet] total_loss : 0.5288873910903931\n",
      "[2021-02-23 21:48:50,123][INFO][TabNet] task_loss : 0.5279220342636108\n",
      "[2021-02-23 21:48:50,124][INFO][TabNet] mask_loss : -0.9653772711753845\n",
      "[2021-02-23 21:48:50,125][INFO][TabNet] time_cost : 0.055642\n",
      "[2021-02-23 21:48:50,126][INFO][TabNet] ******************** epoch : 3 ********************\n",
      "[2021-02-23 21:48:54,055][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:48:54,056][INFO][TabNet] total_loss : 0.4160197079181671\n",
      "[2021-02-23 21:48:54,057][INFO][TabNet] task_loss : 0.4151404798030853\n",
      "[2021-02-23 21:48:54,057][INFO][TabNet] mask_loss : -0.8792352080345154\n",
      "[2021-02-23 21:48:54,058][INFO][TabNet] time_cost : 0.053885\n",
      "[2021-02-23 21:48:54,059][INFO][TabNet] ******************** epoch : 4 ********************\n",
      "[2021-02-23 21:48:58,029][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:48:58,030][INFO][TabNet] total_loss : 0.30468711256980896\n",
      "[2021-02-23 21:48:58,030][INFO][TabNet] task_loss : 0.3038639426231384\n",
      "[2021-02-23 21:48:58,031][INFO][TabNet] mask_loss : -0.8231654167175293\n",
      "[2021-02-23 21:48:58,032][INFO][TabNet] time_cost : 0.053857\n",
      "[2021-02-23 21:48:58,032][INFO][TabNet] ******************** epoch : 5 ********************\n",
      "[2021-02-23 21:49:01,966][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:01,967][INFO][TabNet] total_loss : 0.24533934891223907\n",
      "[2021-02-23 21:49:01,968][INFO][TabNet] task_loss : 0.24459044635295868\n",
      "[2021-02-23 21:49:01,968][INFO][TabNet] mask_loss : -0.7489069700241089\n",
      "[2021-02-23 21:49:01,969][INFO][TabNet] time_cost : 0.053858\n",
      "[2021-02-23 21:49:01,970][INFO][TabNet] ******************** epoch : 6 ********************\n",
      "[2021-02-23 21:49:05,903][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:05,904][INFO][TabNet] total_loss : 0.22546012699604034\n",
      "[2021-02-23 21:49:05,904][INFO][TabNet] task_loss : 0.22479578852653503\n",
      "[2021-02-23 21:49:05,905][INFO][TabNet] mask_loss : -0.6643415689468384\n",
      "[2021-02-23 21:49:05,906][INFO][TabNet] time_cost : 0.053879\n",
      "[2021-02-23 21:49:05,906][INFO][TabNet] ******************** epoch : 7 ********************\n",
      "[2021-02-23 21:49:09,841][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:09,842][INFO][TabNet] total_loss : 0.18420399725437164\n",
      "[2021-02-23 21:49:09,842][INFO][TabNet] task_loss : 0.1835561990737915\n",
      "[2021-02-23 21:49:09,843][INFO][TabNet] mask_loss : -0.647795557975769\n",
      "[2021-02-23 21:49:09,844][INFO][TabNet] time_cost : 0.053845\n",
      "[2021-02-23 21:49:09,845][INFO][TabNet] ******************** epoch : 8 ********************\n",
      "[2021-02-23 21:49:13,780][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:13,781][INFO][TabNet] total_loss : 0.154520645737648\n",
      "[2021-02-23 21:49:13,782][INFO][TabNet] task_loss : 0.15389081835746765\n",
      "[2021-02-23 21:49:13,782][INFO][TabNet] mask_loss : -0.6298230290412903\n",
      "[2021-02-23 21:49:13,783][INFO][TabNet] time_cost : 0.0551\n",
      "[2021-02-23 21:49:13,784][INFO][TabNet] ******************** epoch : 9 ********************\n",
      "[2021-02-23 21:49:17,709][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:17,710][INFO][TabNet] total_loss : 0.20621982216835022\n",
      "[2021-02-23 21:49:17,711][INFO][TabNet] task_loss : 0.2056274116039276\n",
      "[2021-02-23 21:49:17,712][INFO][TabNet] mask_loss : -0.5924174785614014\n",
      "[2021-02-23 21:49:17,713][INFO][TabNet] time_cost : 0.054627\n",
      "[2021-02-23 21:49:17,714][INFO][TabNet] ******************** epoch : 10 ********************\n",
      "[2021-02-23 21:49:21,653][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:21,654][INFO][TabNet] total_loss : 0.2171621173620224\n",
      "[2021-02-23 21:49:21,654][INFO][TabNet] task_loss : 0.21659529209136963\n",
      "[2021-02-23 21:49:21,655][INFO][TabNet] mask_loss : -0.5668295621871948\n",
      "[2021-02-23 21:49:21,656][INFO][TabNet] time_cost : 0.054289\n",
      "[2021-02-23 21:49:21,656][INFO][TabNet] ******************** epoch : 11 ********************\n",
      "[2021-02-23 21:49:25,600][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:25,601][INFO][TabNet] total_loss : 0.17190197110176086\n",
      "[2021-02-23 21:49:25,602][INFO][TabNet] task_loss : 0.17133066058158875\n",
      "[2021-02-23 21:49:25,602][INFO][TabNet] mask_loss : -0.571306586265564\n",
      "[2021-02-23 21:49:25,603][INFO][TabNet] time_cost : 0.05887\n",
      "[2021-02-23 21:49:25,603][INFO][TabNet] ******************** epoch : 12 ********************\n",
      "[2021-02-23 21:49:29,527][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:29,528][INFO][TabNet] total_loss : 0.15011624991893768\n",
      "[2021-02-23 21:49:29,528][INFO][TabNet] task_loss : 0.1495453119277954\n",
      "[2021-02-23 21:49:29,529][INFO][TabNet] mask_loss : -0.5709323287010193\n",
      "[2021-02-23 21:49:29,530][INFO][TabNet] time_cost : 0.053885\n",
      "[2021-02-23 21:49:29,530][INFO][TabNet] ******************** epoch : 13 ********************\n",
      "[2021-02-23 21:49:33,451][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:33,451][INFO][TabNet] total_loss : 0.14784550666809082\n",
      "[2021-02-23 21:49:33,452][INFO][TabNet] task_loss : 0.14728671312332153\n",
      "[2021-02-23 21:49:33,452][INFO][TabNet] mask_loss : -0.5587863326072693\n",
      "[2021-02-23 21:49:33,453][INFO][TabNet] time_cost : 0.05361\n",
      "[2021-02-23 21:49:33,454][INFO][TabNet] ******************** epoch : 14 ********************\n",
      "[2021-02-23 21:49:37,385][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:37,385][INFO][TabNet] total_loss : 0.1300070583820343\n",
      "[2021-02-23 21:49:37,386][INFO][TabNet] task_loss : 0.12947145104408264\n",
      "[2021-02-23 21:49:37,387][INFO][TabNet] mask_loss : -0.535601019859314\n",
      "[2021-02-23 21:49:37,388][INFO][TabNet] time_cost : 0.055308\n",
      "[2021-02-23 21:49:37,388][INFO][TabNet] ******************** epoch : 15 ********************\n",
      "[2021-02-23 21:49:41,317][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:41,317][INFO][TabNet] total_loss : 0.11780089884996414\n",
      "[2021-02-23 21:49:41,318][INFO][TabNet] task_loss : 0.11725912988185883\n",
      "[2021-02-23 21:49:41,319][INFO][TabNet] mask_loss : -0.5417675375938416\n",
      "[2021-02-23 21:49:41,320][INFO][TabNet] time_cost : 0.054877\n",
      "[2021-02-23 21:49:41,321][INFO][TabNet] ******************** epoch : 16 ********************\n",
      "[2021-02-23 21:49:45,246][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:45,246][INFO][TabNet] total_loss : 0.14113737642765045\n",
      "[2021-02-23 21:49:45,247][INFO][TabNet] task_loss : 0.1406238079071045\n",
      "[2021-02-23 21:49:45,248][INFO][TabNet] mask_loss : -0.5135630369186401\n",
      "[2021-02-23 21:49:45,248][INFO][TabNet] time_cost : 0.054826\n",
      "[2021-02-23 21:49:45,249][INFO][TabNet] ******************** epoch : 17 ********************\n",
      "[2021-02-23 21:49:49,170][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:49,171][INFO][TabNet] total_loss : 0.11847619712352753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-23 21:49:49,172][INFO][TabNet] task_loss : 0.11796693503856659\n",
      "[2021-02-23 21:49:49,172][INFO][TabNet] mask_loss : -0.5092649459838867\n",
      "[2021-02-23 21:49:49,173][INFO][TabNet] time_cost : 0.053834\n",
      "[2021-02-23 21:49:49,174][INFO][TabNet] ******************** epoch : 18 ********************\n",
      "[2021-02-23 21:49:53,108][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:53,109][INFO][TabNet] total_loss : 0.1167355552315712\n",
      "[2021-02-23 21:49:53,110][INFO][TabNet] task_loss : 0.11623270064592361\n",
      "[2021-02-23 21:49:53,111][INFO][TabNet] mask_loss : -0.5028560161590576\n",
      "[2021-02-23 21:49:53,111][INFO][TabNet] time_cost : 0.055845\n",
      "[2021-02-23 21:49:53,112][INFO][TabNet] ******************** epoch : 19 ********************\n",
      "[2021-02-23 21:49:57,034][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:49:57,035][INFO][TabNet] total_loss : 0.11471997201442719\n",
      "[2021-02-23 21:49:57,036][INFO][TabNet] task_loss : 0.11424538493156433\n",
      "[2021-02-23 21:49:57,036][INFO][TabNet] mask_loss : -0.47458457946777344\n",
      "[2021-02-23 21:49:57,037][INFO][TabNet] time_cost : 0.054162\n",
      "[2021-02-23 21:49:57,038][INFO][TabNet] ******************** epoch : 20 ********************\n",
      "[2021-02-23 21:50:00,977][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:00,978][INFO][TabNet] total_loss : 0.11732684820890427\n",
      "[2021-02-23 21:50:00,979][INFO][TabNet] task_loss : 0.11688698828220367\n",
      "[2021-02-23 21:50:00,980][INFO][TabNet] mask_loss : -0.4398634135723114\n",
      "[2021-02-23 21:50:00,980][INFO][TabNet] time_cost : 0.061018\n",
      "[2021-02-23 21:50:00,981][INFO][TabNet] ******************** epoch : 21 ********************\n",
      "[2021-02-23 21:50:04,898][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:04,899][INFO][TabNet] total_loss : 0.11340286582708359\n",
      "[2021-02-23 21:50:04,900][INFO][TabNet] task_loss : 0.11301935464143753\n",
      "[2021-02-23 21:50:04,900][INFO][TabNet] mask_loss : -0.3835076689720154\n",
      "[2021-02-23 21:50:04,901][INFO][TabNet] time_cost : 0.053644\n",
      "[2021-02-23 21:50:04,902][INFO][TabNet] ******************** epoch : 22 ********************\n",
      "[2021-02-23 21:50:08,844][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:08,845][INFO][TabNet] total_loss : 0.1187899112701416\n",
      "[2021-02-23 21:50:08,846][INFO][TabNet] task_loss : 0.11841430515050888\n",
      "[2021-02-23 21:50:08,846][INFO][TabNet] mask_loss : -0.37560388445854187\n",
      "[2021-02-23 21:50:08,847][INFO][TabNet] time_cost : 0.060637\n",
      "[2021-02-23 21:50:08,847][INFO][TabNet] ******************** epoch : 23 ********************\n",
      "[2021-02-23 21:50:12,782][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:12,783][INFO][TabNet] total_loss : 0.10212136059999466\n",
      "[2021-02-23 21:50:12,784][INFO][TabNet] task_loss : 0.10176395624876022\n",
      "[2021-02-23 21:50:12,785][INFO][TabNet] mask_loss : -0.3574035167694092\n",
      "[2021-02-23 21:50:12,785][INFO][TabNet] time_cost : 0.055873\n",
      "[2021-02-23 21:50:12,786][INFO][TabNet] ******************** epoch : 24 ********************\n",
      "[2021-02-23 21:50:16,711][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:16,712][INFO][TabNet] total_loss : 0.09683477878570557\n",
      "[2021-02-23 21:50:16,712][INFO][TabNet] task_loss : 0.09649208188056946\n",
      "[2021-02-23 21:50:16,713][INFO][TabNet] mask_loss : -0.3426981568336487\n",
      "[2021-02-23 21:50:16,714][INFO][TabNet] time_cost : 0.054863\n",
      "[2021-02-23 21:50:16,714][INFO][TabNet] ******************** epoch : 25 ********************\n",
      "[2021-02-23 21:50:20,640][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:20,641][INFO][TabNet] total_loss : 0.09234269708395004\n",
      "[2021-02-23 21:50:20,642][INFO][TabNet] task_loss : 0.09201239049434662\n",
      "[2021-02-23 21:50:20,643][INFO][TabNet] mask_loss : -0.33031001687049866\n",
      "[2021-02-23 21:50:20,643][INFO][TabNet] time_cost : 0.053394\n",
      "[2021-02-23 21:50:20,644][INFO][TabNet] ******************** epoch : 26 ********************\n",
      "[2021-02-23 21:50:24,570][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:24,571][INFO][TabNet] total_loss : 0.1070421114563942\n",
      "[2021-02-23 21:50:24,571][INFO][TabNet] task_loss : 0.10669977217912674\n",
      "[2021-02-23 21:50:24,573][INFO][TabNet] mask_loss : -0.34233683347702026\n",
      "[2021-02-23 21:50:24,573][INFO][TabNet] time_cost : 0.053985\n",
      "[2021-02-23 21:50:24,574][INFO][TabNet] ******************** epoch : 27 ********************\n",
      "[2021-02-23 21:50:28,505][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:28,505][INFO][TabNet] total_loss : 0.09537599980831146\n",
      "[2021-02-23 21:50:28,506][INFO][TabNet] task_loss : 0.09507293999195099\n",
      "[2021-02-23 21:50:28,507][INFO][TabNet] mask_loss : -0.3030589520931244\n",
      "[2021-02-23 21:50:28,507][INFO][TabNet] time_cost : 0.055873\n",
      "[2021-02-23 21:50:28,508][INFO][TabNet] ******************** epoch : 28 ********************\n",
      "[2021-02-23 21:50:32,431][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:32,432][INFO][TabNet] total_loss : 0.08002917468547821\n",
      "[2021-02-23 21:50:32,433][INFO][TabNet] task_loss : 0.07975003123283386\n",
      "[2021-02-23 21:50:32,433][INFO][TabNet] mask_loss : -0.2791428565979004\n",
      "[2021-02-23 21:50:32,434][INFO][TabNet] time_cost : 0.052881\n",
      "[2021-02-23 21:50:32,435][INFO][TabNet] ******************** epoch : 29 ********************\n",
      "[2021-02-23 21:50:36,356][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:36,357][INFO][TabNet] total_loss : 0.09682498872280121\n",
      "[2021-02-23 21:50:36,358][INFO][TabNet] task_loss : 0.09655207395553589\n",
      "[2021-02-23 21:50:36,359][INFO][TabNet] mask_loss : -0.27291175723075867\n",
      "[2021-02-23 21:50:36,359][INFO][TabNet] time_cost : 0.054615\n",
      "[2021-02-23 21:50:36,360][INFO][TabNet] ******************** epoch : 30 ********************\n",
      "[2021-02-23 21:50:40,296][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:40,297][INFO][TabNet] total_loss : 0.07783224433660507\n",
      "[2021-02-23 21:50:40,298][INFO][TabNet] task_loss : 0.07756030559539795\n",
      "[2021-02-23 21:50:40,298][INFO][TabNet] mask_loss : -0.271940678358078\n",
      "[2021-02-23 21:50:40,299][INFO][TabNet] time_cost : 0.054883\n",
      "[2021-02-23 21:50:40,300][INFO][TabNet] ******************** epoch : 31 ********************\n",
      "[2021-02-23 21:50:44,244][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:44,245][INFO][TabNet] total_loss : 0.09697596728801727\n",
      "[2021-02-23 21:50:44,245][INFO][TabNet] task_loss : 0.09670563042163849\n",
      "[2021-02-23 21:50:44,246][INFO][TabNet] mask_loss : -0.27033531665802\n",
      "[2021-02-23 21:50:44,247][INFO][TabNet] time_cost : 0.061885\n",
      "[2021-02-23 21:50:44,248][INFO][TabNet] ******************** epoch : 32 ********************\n",
      "[2021-02-23 21:50:48,193][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:48,194][INFO][TabNet] total_loss : 0.10932888835668564\n",
      "[2021-02-23 21:50:48,195][INFO][TabNet] task_loss : 0.10907045006752014\n",
      "[2021-02-23 21:50:48,195][INFO][TabNet] mask_loss : -0.2584405839443207\n",
      "[2021-02-23 21:50:48,196][INFO][TabNet] time_cost : 0.055866\n",
      "[2021-02-23 21:50:48,196][INFO][TabNet] ******************** epoch : 33 ********************\n",
      "[2021-02-23 21:50:52,214][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:52,215][INFO][TabNet] total_loss : 0.07218320667743683\n",
      "[2021-02-23 21:50:52,215][INFO][TabNet] task_loss : 0.07192676514387131\n",
      "[2021-02-23 21:50:52,216][INFO][TabNet] mask_loss : -0.2564385235309601\n",
      "[2021-02-23 21:50:52,216][INFO][TabNet] time_cost : 0.060687\n",
      "[2021-02-23 21:50:52,217][INFO][TabNet] ******************** epoch : 34 ********************\n",
      "[2021-02-23 21:50:56,150][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:50:56,151][INFO][TabNet] total_loss : 0.08611268550157547\n",
      "[2021-02-23 21:50:56,152][INFO][TabNet] task_loss : 0.0858614593744278\n",
      "[2021-02-23 21:50:56,152][INFO][TabNet] mask_loss : -0.25122785568237305\n",
      "[2021-02-23 21:50:56,153][INFO][TabNet] time_cost : 0.054876\n",
      "[2021-02-23 21:50:56,153][INFO][TabNet] ******************** epoch : 35 ********************\n",
      "[2021-02-23 21:51:00,078][INFO][TabNet] -------------------- train info --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-23 21:51:00,078][INFO][TabNet] total_loss : 0.0869387537240982\n",
      "[2021-02-23 21:51:00,079][INFO][TabNet] task_loss : 0.0866905003786087\n",
      "[2021-02-23 21:51:00,080][INFO][TabNet] mask_loss : -0.24825194478034973\n",
      "[2021-02-23 21:51:00,080][INFO][TabNet] time_cost : 0.053805\n",
      "[2021-02-23 21:51:00,081][INFO][TabNet] ******************** epoch : 36 ********************\n",
      "[2021-02-23 21:51:04,013][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:04,014][INFO][TabNet] total_loss : 0.0804620161652565\n",
      "[2021-02-23 21:51:04,015][INFO][TabNet] task_loss : 0.08019043505191803\n",
      "[2021-02-23 21:51:04,016][INFO][TabNet] mask_loss : -0.27158308029174805\n",
      "[2021-02-23 21:51:04,016][INFO][TabNet] time_cost : 0.058864\n",
      "[2021-02-23 21:51:04,018][INFO][TabNet] ******************** epoch : 37 ********************\n",
      "[2021-02-23 21:51:07,951][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:07,952][INFO][TabNet] total_loss : 0.0815630778670311\n",
      "[2021-02-23 21:51:07,953][INFO][TabNet] task_loss : 0.08129072189331055\n",
      "[2021-02-23 21:51:07,953][INFO][TabNet] mask_loss : -0.2723560631275177\n",
      "[2021-02-23 21:51:07,954][INFO][TabNet] time_cost : 0.057855\n",
      "[2021-02-23 21:51:07,955][INFO][TabNet] ******************** epoch : 38 ********************\n",
      "[2021-02-23 21:51:11,889][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:11,890][INFO][TabNet] total_loss : 0.07297563552856445\n",
      "[2021-02-23 21:51:11,890][INFO][TabNet] task_loss : 0.07270166277885437\n",
      "[2021-02-23 21:51:11,891][INFO][TabNet] mask_loss : -0.27397072315216064\n",
      "[2021-02-23 21:51:11,891][INFO][TabNet] time_cost : 0.061182\n",
      "[2021-02-23 21:51:11,892][INFO][TabNet] ******************** epoch : 39 ********************\n",
      "[2021-02-23 21:51:15,817][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:15,818][INFO][TabNet] total_loss : 0.06907360255718231\n",
      "[2021-02-23 21:51:15,819][INFO][TabNet] task_loss : 0.06877891719341278\n",
      "[2021-02-23 21:51:15,820][INFO][TabNet] mask_loss : -0.29468199610710144\n",
      "[2021-02-23 21:51:15,820][INFO][TabNet] time_cost : 0.055835\n",
      "[2021-02-23 21:51:15,821][INFO][TabNet] ******************** epoch : 40 ********************\n",
      "[2021-02-23 21:51:19,746][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:19,747][INFO][TabNet] total_loss : 0.06821589171886444\n",
      "[2021-02-23 21:51:19,747][INFO][TabNet] task_loss : 0.06790737807750702\n",
      "[2021-02-23 21:51:19,748][INFO][TabNet] mask_loss : -0.3085155487060547\n",
      "[2021-02-23 21:51:19,749][INFO][TabNet] time_cost : 0.063868\n",
      "[2021-02-23 21:51:19,750][INFO][TabNet] ******************** epoch : 41 ********************\n",
      "[2021-02-23 21:51:23,681][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:23,682][INFO][TabNet] total_loss : 0.09378542006015778\n",
      "[2021-02-23 21:51:23,683][INFO][TabNet] task_loss : 0.09347544610500336\n",
      "[2021-02-23 21:51:23,684][INFO][TabNet] mask_loss : -0.30997300148010254\n",
      "[2021-02-23 21:51:23,684][INFO][TabNet] time_cost : 0.065084\n",
      "[2021-02-23 21:51:23,685][INFO][TabNet] ******************** epoch : 42 ********************\n",
      "[2021-02-23 21:51:27,613][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:27,614][INFO][TabNet] total_loss : 0.0615086629986763\n",
      "[2021-02-23 21:51:27,615][INFO][TabNet] task_loss : 0.06119082123041153\n",
      "[2021-02-23 21:51:27,615][INFO][TabNet] mask_loss : -0.317841112613678\n",
      "[2021-02-23 21:51:27,616][INFO][TabNet] time_cost : 0.053887\n",
      "[2021-02-23 21:51:27,616][INFO][TabNet] ******************** epoch : 43 ********************\n",
      "[2021-02-23 21:51:31,551][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:31,552][INFO][TabNet] total_loss : 0.06754539161920547\n",
      "[2021-02-23 21:51:31,552][INFO][TabNet] task_loss : 0.06723056733608246\n",
      "[2021-02-23 21:51:31,553][INFO][TabNet] mask_loss : -0.3148270845413208\n",
      "[2021-02-23 21:51:31,553][INFO][TabNet] time_cost : 0.060712\n",
      "[2021-02-23 21:51:31,554][INFO][TabNet] ******************** epoch : 44 ********************\n",
      "[2021-02-23 21:51:35,471][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:35,472][INFO][TabNet] total_loss : 0.055131055414676666\n",
      "[2021-02-23 21:51:35,473][INFO][TabNet] task_loss : 0.054802246391773224\n",
      "[2021-02-23 21:51:35,473][INFO][TabNet] mask_loss : -0.3288097381591797\n",
      "[2021-02-23 21:51:35,474][INFO][TabNet] time_cost : 0.054258\n",
      "[2021-02-23 21:51:35,475][INFO][TabNet] ******************** epoch : 45 ********************\n",
      "[2021-02-23 21:51:39,390][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:39,391][INFO][TabNet] total_loss : 0.07645363360643387\n",
      "[2021-02-23 21:51:39,392][INFO][TabNet] task_loss : 0.0761178582906723\n",
      "[2021-02-23 21:51:39,392][INFO][TabNet] mask_loss : -0.33577367663383484\n",
      "[2021-02-23 21:51:39,393][INFO][TabNet] time_cost : 0.053495\n",
      "[2021-02-23 21:51:39,393][INFO][TabNet] ******************** epoch : 46 ********************\n",
      "[2021-02-23 21:51:43,319][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:43,319][INFO][TabNet] total_loss : 0.07184545695781708\n",
      "[2021-02-23 21:51:43,320][INFO][TabNet] task_loss : 0.07151356339454651\n",
      "[2021-02-23 21:51:43,321][INFO][TabNet] mask_loss : -0.331895112991333\n",
      "[2021-02-23 21:51:43,322][INFO][TabNet] time_cost : 0.054848\n",
      "[2021-02-23 21:51:43,322][INFO][TabNet] ******************** epoch : 47 ********************\n",
      "[2021-02-23 21:51:47,246][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:47,247][INFO][TabNet] total_loss : 0.07012429088354111\n",
      "[2021-02-23 21:51:47,248][INFO][TabNet] task_loss : 0.06979605555534363\n",
      "[2021-02-23 21:51:47,249][INFO][TabNet] mask_loss : -0.3282352685928345\n",
      "[2021-02-23 21:51:47,249][INFO][TabNet] time_cost : 0.05488\n",
      "[2021-02-23 21:51:47,250][INFO][TabNet] ******************** epoch : 48 ********************\n",
      "[2021-02-23 21:51:51,157][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:51,159][INFO][TabNet] total_loss : 0.06696385145187378\n",
      "[2021-02-23 21:51:51,159][INFO][TabNet] task_loss : 0.06662238389253616\n",
      "[2021-02-23 21:51:51,160][INFO][TabNet] mask_loss : -0.34146732091903687\n",
      "[2021-02-23 21:51:51,161][INFO][TabNet] time_cost : 0.052885\n",
      "[2021-02-23 21:51:51,161][INFO][TabNet] ******************** epoch : 49 ********************\n",
      "[2021-02-23 21:51:55,090][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:55,091][INFO][TabNet] total_loss : 0.08520005643367767\n",
      "[2021-02-23 21:51:55,091][INFO][TabNet] task_loss : 0.08485344797372818\n",
      "[2021-02-23 21:51:55,092][INFO][TabNet] mask_loss : -0.3466080129146576\n",
      "[2021-02-23 21:51:55,093][INFO][TabNet] time_cost : 0.054035\n",
      "[2021-02-23 21:51:55,094][INFO][TabNet] ******************** epoch : 50 ********************\n",
      "[2021-02-23 21:51:59,018][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:51:59,019][INFO][TabNet] total_loss : 0.05510592833161354\n",
      "[2021-02-23 21:51:59,020][INFO][TabNet] task_loss : 0.0547812320291996\n",
      "[2021-02-23 21:51:59,020][INFO][TabNet] mask_loss : -0.3246944546699524\n",
      "[2021-02-23 21:51:59,021][INFO][TabNet] time_cost : 0.054874\n",
      "[2021-02-23 21:51:59,022][INFO][TabNet] ******************** epoch : 51 ********************\n",
      "[2021-02-23 21:52:03,006][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:03,007][INFO][TabNet] total_loss : 0.06457877904176712\n",
      "[2021-02-23 21:52:03,008][INFO][TabNet] task_loss : 0.06424206495285034\n",
      "[2021-02-23 21:52:03,009][INFO][TabNet] mask_loss : -0.336710661649704\n",
      "[2021-02-23 21:52:03,009][INFO][TabNet] time_cost : 0.055851\n",
      "[2021-02-23 21:52:03,010][INFO][TabNet] ******************** epoch : 52 ********************\n",
      "[2021-02-23 21:52:07,000][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:07,001][INFO][TabNet] total_loss : 0.05756136402487755\n",
      "[2021-02-23 21:52:07,001][INFO][TabNet] task_loss : 0.05722268670797348\n",
      "[2021-02-23 21:52:07,002][INFO][TabNet] mask_loss : -0.33867549896240234\n",
      "[2021-02-23 21:52:07,002][INFO][TabNet] time_cost : 0.054874\n",
      "[2021-02-23 21:52:07,003][INFO][TabNet] ******************** epoch : 53 ********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-23 21:52:10,970][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:10,971][INFO][TabNet] total_loss : 0.06692496687173843\n",
      "[2021-02-23 21:52:10,972][INFO][TabNet] task_loss : 0.06660980731248856\n",
      "[2021-02-23 21:52:10,973][INFO][TabNet] mask_loss : -0.3151610493659973\n",
      "[2021-02-23 21:52:10,974][INFO][TabNet] time_cost : 0.060703\n",
      "[2021-02-23 21:52:10,975][INFO][TabNet] ******************** epoch : 54 ********************\n",
      "[2021-02-23 21:52:14,907][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:14,908][INFO][TabNet] total_loss : 0.06917443871498108\n",
      "[2021-02-23 21:52:14,909][INFO][TabNet] task_loss : 0.06884706020355225\n",
      "[2021-02-23 21:52:14,909][INFO][TabNet] mask_loss : -0.32737916707992554\n",
      "[2021-02-23 21:52:14,910][INFO][TabNet] time_cost : 0.053853\n",
      "[2021-02-23 21:52:14,910][INFO][TabNet] ******************** epoch : 55 ********************\n",
      "[2021-02-23 21:52:18,826][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:18,827][INFO][TabNet] total_loss : 0.060226667672395706\n",
      "[2021-02-23 21:52:18,828][INFO][TabNet] task_loss : 0.059897635132074356\n",
      "[2021-02-23 21:52:18,828][INFO][TabNet] mask_loss : -0.3290335536003113\n",
      "[2021-02-23 21:52:18,829][INFO][TabNet] time_cost : 0.053885\n",
      "[2021-02-23 21:52:18,829][INFO][TabNet] ******************** epoch : 56 ********************\n",
      "[2021-02-23 21:52:22,756][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:22,757][INFO][TabNet] total_loss : 0.08046556264162064\n",
      "[2021-02-23 21:52:22,758][INFO][TabNet] task_loss : 0.08012551814317703\n",
      "[2021-02-23 21:52:22,759][INFO][TabNet] mask_loss : -0.34004318714141846\n",
      "[2021-02-23 21:52:22,759][INFO][TabNet] time_cost : 0.053734\n",
      "[2021-02-23 21:52:22,760][INFO][TabNet] ******************** epoch : 57 ********************\n",
      "[2021-02-23 21:52:26,691][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:26,691][INFO][TabNet] total_loss : 0.06815966218709946\n",
      "[2021-02-23 21:52:26,692][INFO][TabNet] task_loss : 0.06781776994466782\n",
      "[2021-02-23 21:52:26,693][INFO][TabNet] mask_loss : -0.34188970923423767\n",
      "[2021-02-23 21:52:26,694][INFO][TabNet] time_cost : 0.061044\n",
      "[2021-02-23 21:52:26,694][INFO][TabNet] ******************** epoch : 58 ********************\n",
      "[2021-02-23 21:52:30,610][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:30,611][INFO][TabNet] total_loss : 0.06577444076538086\n",
      "[2021-02-23 21:52:30,611][INFO][TabNet] task_loss : 0.06544196605682373\n",
      "[2021-02-23 21:52:30,612][INFO][TabNet] mask_loss : -0.3324747085571289\n",
      "[2021-02-23 21:52:30,613][INFO][TabNet] time_cost : 0.053883\n",
      "[2021-02-23 21:52:30,613][INFO][TabNet] ******************** epoch : 59 ********************\n",
      "[2021-02-23 21:52:34,534][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:34,535][INFO][TabNet] total_loss : 0.053378500044345856\n",
      "[2021-02-23 21:52:34,536][INFO][TabNet] task_loss : 0.05303032323718071\n",
      "[2021-02-23 21:52:34,536][INFO][TabNet] mask_loss : -0.34817737340927124\n",
      "[2021-02-23 21:52:34,537][INFO][TabNet] time_cost : 0.053885\n",
      "[2021-02-23 21:52:34,537][INFO][TabNet] ******************** epoch : 60 ********************\n",
      "[2021-02-23 21:52:38,458][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:38,459][INFO][TabNet] total_loss : 0.05757177248597145\n",
      "[2021-02-23 21:52:38,459][INFO][TabNet] task_loss : 0.057222556322813034\n",
      "[2021-02-23 21:52:38,460][INFO][TabNet] mask_loss : -0.3492144048213959\n",
      "[2021-02-23 21:52:38,460][INFO][TabNet] time_cost : 0.054849\n",
      "[2021-02-23 21:52:38,461][INFO][TabNet] ******************** epoch : 61 ********************\n",
      "[2021-02-23 21:52:42,402][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:42,403][INFO][TabNet] total_loss : 0.05581852048635483\n",
      "[2021-02-23 21:52:42,403][INFO][TabNet] task_loss : 0.05546276643872261\n",
      "[2021-02-23 21:52:42,404][INFO][TabNet] mask_loss : -0.35575294494628906\n",
      "[2021-02-23 21:52:42,405][INFO][TabNet] time_cost : 0.053781\n",
      "[2021-02-23 21:52:42,405][INFO][TabNet] ******************** epoch : 62 ********************\n",
      "[2021-02-23 21:52:46,330][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:46,331][INFO][TabNet] total_loss : 0.0552930012345314\n",
      "[2021-02-23 21:52:46,332][INFO][TabNet] task_loss : 0.05494924634695053\n",
      "[2021-02-23 21:52:46,333][INFO][TabNet] mask_loss : -0.34375447034835815\n",
      "[2021-02-23 21:52:46,333][INFO][TabNet] time_cost : 0.060867\n",
      "[2021-02-23 21:52:46,334][INFO][TabNet] ******************** epoch : 63 ********************\n",
      "[2021-02-23 21:52:50,251][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:50,252][INFO][TabNet] total_loss : 0.05810421705245972\n",
      "[2021-02-23 21:52:50,252][INFO][TabNet] task_loss : 0.0577632300555706\n",
      "[2021-02-23 21:52:50,253][INFO][TabNet] mask_loss : -0.3409883677959442\n",
      "[2021-02-23 21:52:50,254][INFO][TabNet] time_cost : 0.053714\n",
      "[2021-02-23 21:52:50,254][INFO][TabNet] ******************** epoch : 64 ********************\n",
      "[2021-02-23 21:52:54,174][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:54,175][INFO][TabNet] total_loss : 0.047822512686252594\n",
      "[2021-02-23 21:52:54,175][INFO][TabNet] task_loss : 0.04748509079217911\n",
      "[2021-02-23 21:52:54,176][INFO][TabNet] mask_loss : -0.3374222218990326\n",
      "[2021-02-23 21:52:54,177][INFO][TabNet] time_cost : 0.054885\n",
      "[2021-02-23 21:52:54,177][INFO][TabNet] ******************** epoch : 65 ********************\n",
      "[2021-02-23 21:52:58,102][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:52:58,103][INFO][TabNet] total_loss : 0.054068513214588165\n",
      "[2021-02-23 21:52:58,104][INFO][TabNet] task_loss : 0.053732022643089294\n",
      "[2021-02-23 21:52:58,104][INFO][TabNet] mask_loss : -0.3364895284175873\n",
      "[2021-02-23 21:52:58,105][INFO][TabNet] time_cost : 0.055285\n",
      "[2021-02-23 21:52:58,106][INFO][TabNet] ******************** epoch : 66 ********************\n",
      "[2021-02-23 21:53:02,032][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:02,033][INFO][TabNet] total_loss : 0.06213231757283211\n",
      "[2021-02-23 21:53:02,034][INFO][TabNet] task_loss : 0.06181362643837929\n",
      "[2021-02-23 21:53:02,034][INFO][TabNet] mask_loss : -0.31869277358055115\n",
      "[2021-02-23 21:53:02,035][INFO][TabNet] time_cost : 0.054249\n",
      "[2021-02-23 21:53:02,035][INFO][TabNet] ******************** epoch : 67 ********************\n",
      "[2021-02-23 21:53:05,962][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:05,962][INFO][TabNet] total_loss : 0.0463571771979332\n",
      "[2021-02-23 21:53:05,963][INFO][TabNet] task_loss : 0.046058639883995056\n",
      "[2021-02-23 21:53:05,963][INFO][TabNet] mask_loss : -0.29853567481040955\n",
      "[2021-02-23 21:53:05,964][INFO][TabNet] time_cost : 0.053899\n",
      "[2021-02-23 21:53:05,965][INFO][TabNet] ******************** epoch : 68 ********************\n",
      "[2021-02-23 21:53:09,891][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:09,892][INFO][TabNet] total_loss : 0.05885130167007446\n",
      "[2021-02-23 21:53:09,893][INFO][TabNet] task_loss : 0.05855932831764221\n",
      "[2021-02-23 21:53:09,893][INFO][TabNet] mask_loss : -0.29197296500205994\n",
      "[2021-02-23 21:53:09,894][INFO][TabNet] time_cost : 0.053868\n",
      "[2021-02-23 21:53:09,895][INFO][TabNet] ******************** epoch : 69 ********************\n",
      "[2021-02-23 21:53:13,817][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:13,818][INFO][TabNet] total_loss : 0.053571783006191254\n",
      "[2021-02-23 21:53:13,819][INFO][TabNet] task_loss : 0.0532965287566185\n",
      "[2021-02-23 21:53:13,820][INFO][TabNet] mask_loss : -0.2752555310726166\n",
      "[2021-02-23 21:53:13,820][INFO][TabNet] time_cost : 0.052863\n",
      "[2021-02-23 21:53:13,821][INFO][TabNet] ******************** epoch : 70 ********************\n",
      "[2021-02-23 21:53:17,749][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:17,750][INFO][TabNet] total_loss : 0.05181054398417473\n",
      "[2021-02-23 21:53:17,750][INFO][TabNet] task_loss : 0.051527611911296844\n",
      "[2021-02-23 21:53:17,751][INFO][TabNet] mask_loss : -0.28293371200561523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-23 21:53:17,751][INFO][TabNet] time_cost : 0.053881\n",
      "[2021-02-23 21:53:17,752][INFO][TabNet] ******************** epoch : 71 ********************\n",
      "[2021-02-23 21:53:21,668][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:21,669][INFO][TabNet] total_loss : 0.05028965696692467\n",
      "[2021-02-23 21:53:21,669][INFO][TabNet] task_loss : 0.050007425248622894\n",
      "[2021-02-23 21:53:21,670][INFO][TabNet] mask_loss : -0.2822331190109253\n",
      "[2021-02-23 21:53:21,670][INFO][TabNet] time_cost : 0.053858\n",
      "[2021-02-23 21:53:21,671][INFO][TabNet] ******************** epoch : 72 ********************\n",
      "[2021-02-23 21:53:25,602][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:25,603][INFO][TabNet] total_loss : 0.04899831861257553\n",
      "[2021-02-23 21:53:25,604][INFO][TabNet] task_loss : 0.04872569441795349\n",
      "[2021-02-23 21:53:25,605][INFO][TabNet] mask_loss : -0.272623747587204\n",
      "[2021-02-23 21:53:25,606][INFO][TabNet] time_cost : 0.05448\n",
      "[2021-02-23 21:53:25,606][INFO][TabNet] ******************** epoch : 73 ********************\n",
      "[2021-02-23 21:53:29,535][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:29,536][INFO][TabNet] total_loss : 0.05797462910413742\n",
      "[2021-02-23 21:53:29,536][INFO][TabNet] task_loss : 0.057703614234924316\n",
      "[2021-02-23 21:53:29,537][INFO][TabNet] mask_loss : -0.2710133492946625\n",
      "[2021-02-23 21:53:29,538][INFO][TabNet] time_cost : 0.055808\n",
      "[2021-02-23 21:53:29,538][INFO][TabNet] ******************** epoch : 74 ********************\n",
      "[2021-02-23 21:53:33,448][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:33,449][INFO][TabNet] total_loss : 0.04466460645198822\n",
      "[2021-02-23 21:53:33,449][INFO][TabNet] task_loss : 0.04439825564622879\n",
      "[2021-02-23 21:53:33,450][INFO][TabNet] mask_loss : -0.26635122299194336\n",
      "[2021-02-23 21:53:33,451][INFO][TabNet] time_cost : 0.053886\n",
      "[2021-02-23 21:53:33,451][INFO][TabNet] ******************** epoch : 75 ********************\n",
      "[2021-02-23 21:53:37,381][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:37,382][INFO][TabNet] total_loss : 0.04483729228377342\n",
      "[2021-02-23 21:53:37,383][INFO][TabNet] task_loss : 0.04457249492406845\n",
      "[2021-02-23 21:53:37,384][INFO][TabNet] mask_loss : -0.264797568321228\n",
      "[2021-02-23 21:53:37,385][INFO][TabNet] time_cost : 0.062075\n",
      "[2021-02-23 21:53:37,386][INFO][TabNet] ******************** epoch : 76 ********************\n",
      "[2021-02-23 21:53:41,309][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:41,310][INFO][TabNet] total_loss : 0.03373940661549568\n",
      "[2021-02-23 21:53:41,310][INFO][TabNet] task_loss : 0.033481113612651825\n",
      "[2021-02-23 21:53:41,311][INFO][TabNet] mask_loss : -0.2582933306694031\n",
      "[2021-02-23 21:53:41,311][INFO][TabNet] time_cost : 0.05486\n",
      "[2021-02-23 21:53:41,312][INFO][TabNet] ******************** epoch : 77 ********************\n",
      "[2021-02-23 21:53:45,242][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:45,243][INFO][TabNet] total_loss : 0.05737736076116562\n",
      "[2021-02-23 21:53:45,244][INFO][TabNet] task_loss : 0.05710716173052788\n",
      "[2021-02-23 21:53:45,244][INFO][TabNet] mask_loss : -0.27020031213760376\n",
      "[2021-02-23 21:53:45,245][INFO][TabNet] time_cost : 0.053096\n",
      "[2021-02-23 21:53:45,246][INFO][TabNet] ******************** epoch : 78 ********************\n",
      "[2021-02-23 21:53:49,171][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:49,172][INFO][TabNet] total_loss : 0.06181842088699341\n",
      "[2021-02-23 21:53:49,173][INFO][TabNet] task_loss : 0.06155890226364136\n",
      "[2021-02-23 21:53:49,174][INFO][TabNet] mask_loss : -0.25952038168907166\n",
      "[2021-02-23 21:53:49,174][INFO][TabNet] time_cost : 0.053709\n",
      "[2021-02-23 21:53:49,175][INFO][TabNet] ******************** epoch : 79 ********************\n",
      "[2021-02-23 21:53:53,103][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:53,104][INFO][TabNet] total_loss : 0.04723212122917175\n",
      "[2021-02-23 21:53:53,105][INFO][TabNet] task_loss : 0.046974923461675644\n",
      "[2021-02-23 21:53:53,105][INFO][TabNet] mask_loss : -0.25719842314720154\n",
      "[2021-02-23 21:53:53,106][INFO][TabNet] time_cost : 0.05388\n",
      "[2021-02-23 21:53:53,107][INFO][TabNet] ******************** epoch : 80 ********************\n",
      "[2021-02-23 21:53:57,156][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:53:57,157][INFO][TabNet] total_loss : 0.062089357525110245\n",
      "[2021-02-23 21:53:57,157][INFO][TabNet] task_loss : 0.06182987987995148\n",
      "[2021-02-23 21:53:57,158][INFO][TabNet] mask_loss : -0.2594790458679199\n",
      "[2021-02-23 21:53:57,158][INFO][TabNet] time_cost : 0.053885\n",
      "[2021-02-23 21:53:57,159][INFO][TabNet] ******************** epoch : 81 ********************\n",
      "[2021-02-23 21:54:01,092][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:01,093][INFO][TabNet] total_loss : 0.028430650010704994\n",
      "[2021-02-23 21:54:01,094][INFO][TabNet] task_loss : 0.028170131146907806\n",
      "[2021-02-23 21:54:01,094][INFO][TabNet] mask_loss : -0.2605185806751251\n",
      "[2021-02-23 21:54:01,095][INFO][TabNet] time_cost : 0.053577\n",
      "[2021-02-23 21:54:01,096][INFO][TabNet] ******************** epoch : 82 ********************\n",
      "[2021-02-23 21:54:05,022][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:05,023][INFO][TabNet] total_loss : 0.05090031772851944\n",
      "[2021-02-23 21:54:05,024][INFO][TabNet] task_loss : 0.05064069479703903\n",
      "[2021-02-23 21:54:05,024][INFO][TabNet] mask_loss : -0.2596243917942047\n",
      "[2021-02-23 21:54:05,025][INFO][TabNet] time_cost : 0.0534\n",
      "[2021-02-23 21:54:05,025][INFO][TabNet] ******************** epoch : 83 ********************\n",
      "[2021-02-23 21:54:08,947][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:08,948][INFO][TabNet] total_loss : 0.05011007934808731\n",
      "[2021-02-23 21:54:08,949][INFO][TabNet] task_loss : 0.049847312271595\n",
      "[2021-02-23 21:54:08,950][INFO][TabNet] mask_loss : -0.26276707649230957\n",
      "[2021-02-23 21:54:08,950][INFO][TabNet] time_cost : 0.054888\n",
      "[2021-02-23 21:54:08,951][INFO][TabNet] ******************** epoch : 84 ********************\n",
      "[2021-02-23 21:54:12,878][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:12,879][INFO][TabNet] total_loss : 0.054155368357896805\n",
      "[2021-02-23 21:54:12,880][INFO][TabNet] task_loss : 0.05389288067817688\n",
      "[2021-02-23 21:54:12,881][INFO][TabNet] mask_loss : -0.2624891698360443\n",
      "[2021-02-23 21:54:12,881][INFO][TabNet] time_cost : 0.054865\n",
      "[2021-02-23 21:54:12,882][INFO][TabNet] ******************** epoch : 85 ********************\n",
      "[2021-02-23 21:54:16,817][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:16,818][INFO][TabNet] total_loss : 0.04889262095093727\n",
      "[2021-02-23 21:54:16,819][INFO][TabNet] task_loss : 0.048630423843860626\n",
      "[2021-02-23 21:54:16,820][INFO][TabNet] mask_loss : -0.26219677925109863\n",
      "[2021-02-23 21:54:16,820][INFO][TabNet] time_cost : 0.053718\n",
      "[2021-02-23 21:54:16,821][INFO][TabNet] ******************** epoch : 86 ********************\n",
      "[2021-02-23 21:54:20,762][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:20,763][INFO][TabNet] total_loss : 0.03725368157029152\n",
      "[2021-02-23 21:54:20,764][INFO][TabNet] task_loss : 0.03698643296957016\n",
      "[2021-02-23 21:54:20,765][INFO][TabNet] mask_loss : -0.2672470211982727\n",
      "[2021-02-23 21:54:20,766][INFO][TabNet] time_cost : 0.060867\n",
      "[2021-02-23 21:54:20,766][INFO][TabNet] ******************** epoch : 87 ********************\n",
      "[2021-02-23 21:54:24,705][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:24,706][INFO][TabNet] total_loss : 0.04082310572266579\n",
      "[2021-02-23 21:54:24,707][INFO][TabNet] task_loss : 0.04055051505565643\n",
      "[2021-02-23 21:54:24,707][INFO][TabNet] mask_loss : -0.27259066700935364\n",
      "[2021-02-23 21:54:24,708][INFO][TabNet] time_cost : 0.053886\n",
      "[2021-02-23 21:54:24,709][INFO][TabNet] ******************** epoch : 88 ********************\n",
      "[2021-02-23 21:54:28,646][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:28,647][INFO][TabNet] total_loss : 0.05641373246908188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-23 21:54:28,648][INFO][TabNet] task_loss : 0.056136827915906906\n",
      "[2021-02-23 21:54:28,648][INFO][TabNet] mask_loss : -0.2769050896167755\n",
      "[2021-02-23 21:54:28,649][INFO][TabNet] time_cost : 0.054883\n",
      "[2021-02-23 21:54:28,649][INFO][TabNet] ******************** epoch : 89 ********************\n",
      "[2021-02-23 21:54:32,578][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:32,579][INFO][TabNet] total_loss : 0.053862135857343674\n",
      "[2021-02-23 21:54:32,580][INFO][TabNet] task_loss : 0.05357220768928528\n",
      "[2021-02-23 21:54:32,580][INFO][TabNet] mask_loss : -0.2899284362792969\n",
      "[2021-02-23 21:54:32,581][INFO][TabNet] time_cost : 0.054875\n",
      "[2021-02-23 21:54:32,581][INFO][TabNet] ******************** epoch : 90 ********************\n",
      "[2021-02-23 21:54:36,509][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:36,510][INFO][TabNet] total_loss : 0.041292257606983185\n",
      "[2021-02-23 21:54:36,511][INFO][TabNet] task_loss : 0.04100649803876877\n",
      "[2021-02-23 21:54:36,511][INFO][TabNet] mask_loss : -0.28575843572616577\n",
      "[2021-02-23 21:54:36,512][INFO][TabNet] time_cost : 0.053848\n",
      "[2021-02-23 21:54:36,513][INFO][TabNet] ******************** epoch : 91 ********************\n",
      "[2021-02-23 21:54:40,441][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:40,442][INFO][TabNet] total_loss : 0.055361609905958176\n",
      "[2021-02-23 21:54:40,442][INFO][TabNet] task_loss : 0.05507444590330124\n",
      "[2021-02-23 21:54:40,443][INFO][TabNet] mask_loss : -0.2871641218662262\n",
      "[2021-02-23 21:54:40,444][INFO][TabNet] time_cost : 0.054026\n",
      "[2021-02-23 21:54:40,444][INFO][TabNet] ******************** epoch : 92 ********************\n",
      "[2021-02-23 21:54:44,364][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:44,365][INFO][TabNet] total_loss : 0.04725393280386925\n",
      "[2021-02-23 21:54:44,366][INFO][TabNet] task_loss : 0.046955808997154236\n",
      "[2021-02-23 21:54:44,366][INFO][TabNet] mask_loss : -0.29812365770339966\n",
      "[2021-02-23 21:54:44,367][INFO][TabNet] time_cost : 0.054001\n",
      "[2021-02-23 21:54:44,368][INFO][TabNet] ******************** epoch : 93 ********************\n",
      "[2021-02-23 21:54:48,285][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:48,286][INFO][TabNet] total_loss : 0.035607337951660156\n",
      "[2021-02-23 21:54:48,286][INFO][TabNet] task_loss : 0.0352974608540535\n",
      "[2021-02-23 21:54:48,287][INFO][TabNet] mask_loss : -0.30987539887428284\n",
      "[2021-02-23 21:54:48,288][INFO][TabNet] time_cost : 0.052859\n",
      "[2021-02-23 21:54:48,288][INFO][TabNet] ******************** epoch : 94 ********************\n",
      "[2021-02-23 21:54:52,214][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:52,215][INFO][TabNet] total_loss : 0.04093797504901886\n",
      "[2021-02-23 21:54:52,216][INFO][TabNet] task_loss : 0.04063882306218147\n",
      "[2021-02-23 21:54:52,217][INFO][TabNet] mask_loss : -0.2991529703140259\n",
      "[2021-02-23 21:54:52,217][INFO][TabNet] time_cost : 0.062855\n",
      "[2021-02-23 21:54:52,218][INFO][TabNet] ******************** epoch : 95 ********************\n",
      "[2021-02-23 21:54:56,150][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:54:56,151][INFO][TabNet] total_loss : 0.04424683749675751\n",
      "[2021-02-23 21:54:56,152][INFO][TabNet] task_loss : 0.0439586378633976\n",
      "[2021-02-23 21:54:56,152][INFO][TabNet] mask_loss : -0.28820106387138367\n",
      "[2021-02-23 21:54:56,153][INFO][TabNet] time_cost : 0.053883\n",
      "[2021-02-23 21:54:56,153][INFO][TabNet] ******************** epoch : 96 ********************\n",
      "[2021-02-23 21:55:00,081][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:55:00,082][INFO][TabNet] total_loss : 0.05662672221660614\n",
      "[2021-02-23 21:55:00,083][INFO][TabNet] task_loss : 0.056329455226659775\n",
      "[2021-02-23 21:55:00,084][INFO][TabNet] mask_loss : -0.29726681113243103\n",
      "[2021-02-23 21:55:00,084][INFO][TabNet] time_cost : 0.054756\n",
      "[2021-02-23 21:55:00,085][INFO][TabNet] ******************** epoch : 97 ********************\n",
      "[2021-02-23 21:55:04,010][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:55:04,011][INFO][TabNet] total_loss : 0.036332376301288605\n",
      "[2021-02-23 21:55:04,012][INFO][TabNet] task_loss : 0.03603322431445122\n",
      "[2021-02-23 21:55:04,012][INFO][TabNet] mask_loss : -0.299152135848999\n",
      "[2021-02-23 21:55:04,013][INFO][TabNet] time_cost : 0.053858\n",
      "[2021-02-23 21:55:04,014][INFO][TabNet] ******************** epoch : 98 ********************\n",
      "[2021-02-23 21:55:07,941][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:55:07,942][INFO][TabNet] total_loss : 0.055137913674116135\n",
      "[2021-02-23 21:55:07,942][INFO][TabNet] task_loss : 0.054844390600919724\n",
      "[2021-02-23 21:55:07,943][INFO][TabNet] mask_loss : -0.29352158308029175\n",
      "[2021-02-23 21:55:07,944][INFO][TabNet] time_cost : 0.053765\n",
      "[2021-02-23 21:55:07,945][INFO][TabNet] ******************** epoch : 99 ********************\n",
      "[2021-02-23 21:55:11,861][INFO][TabNet] -------------------- train info --------------------\n",
      "[2021-02-23 21:55:11,862][INFO][TabNet] total_loss : 0.0547923780977726\n",
      "[2021-02-23 21:55:11,862][INFO][TabNet] task_loss : 0.05450128763914108\n",
      "[2021-02-23 21:55:11,863][INFO][TabNet] mask_loss : -0.2910888195037842\n",
      "[2021-02-23 21:55:11,864][INFO][TabNet] time_cost : 0.055843\n",
      "[2021-02-23 21:55:11,864][INFO][TabNet] training complete.\n",
      "[2021-02-23 21:55:11,865][INFO][TabNet] ******************** Summary Info ********************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomizedEstimator(input_dims=30, output_dims=[1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.fit(X, y.reshape(-1, 1), **training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9595782073813708"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = my_model.predict(X)\n",
    "accuracy_score(y_pred[0], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
