{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnet.utils.logger import init_logger\n",
    "from tabnet.estimator import TabNetRegressor\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_dir = 'logs'\n",
    "logger_name = 'TestRegression'\n",
    "level = 'INFO'\n",
    "\n",
    "logger = init_logger(logger_dir=logger_dir, logger_name=logger_name, level=level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet = TabNetRegressor(\n",
    "    input_dims=13, output_dims=[1], logger=logger, is_cuda=False,\n",
    "    reprs_dims=4, atten_dims=4, num_steps=3, num_indep=1, num_shared=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-19 08:49:56,149][WARNING][TabNet] Failed to load model from None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TabNetRegressor(atten_dims=4, batch_size=1024, cate_dims=None,\n",
       "                cate_embed_dims=1, cate_indices=None, criterions=['mse'],\n",
       "                gamma=1.3, input_dims=13, is_cuda=False, is_shuffle=True,\n",
       "                logger=<RootLogger root (INFO)>, mask_type='sparsemax',\n",
       "                momentum=0.03, num_indep=1, num_shared=1, num_steps=3,\n",
       "                num_workers=4, output_dims=[1], pin_memory=True, reprs_dims=4,\n",
       "                task_weights=1, virtual_batch_size=128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabnet.build(path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "training_params = {\n",
    "    'batch_size': 256,\n",
    "    'max_epochs': 200,\n",
    "    'metrics': ['mse'],\n",
    "    'optimizer': Adam,\n",
    "    'optimizer_params': {'lr': 0.1},\n",
    "    'schedulers': [lr_scheduler.ExponentialLR],\n",
    "    'scheduler_params': {'gamma': 0.99}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet.fit(X, y.reshape(-1, 1), **training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance, masks = tabnet.explain(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 1, figsize=(5,15))\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].imshow(masks[i].cpu().numpy()[:20])\n",
    "#     axs[i].set_xlabel('features')\n",
    "    axs[i].set_ylabel('samples')\n",
    "    axs[i].set_title(f\"mask {i}\")\n",
    "    axs[i].set_yticks(range(20))\n",
    "#     axs[i].set_xticks(range(30))\n",
    "#     axs[i].set_xticklabels(feature_names, rotation=90)\n",
    "\n",
    "axs[3].imshow(importance.cpu().numpy()[:20, :])\n",
    "axs[3].set_xlabel('features')\n",
    "axs[3].set_ylabel('samples')\n",
    "axs[3].set_title('importance')\n",
    "axs[3].set_yticks(range(20))\n",
    "axs[3].set_xticks(range(13))\n",
    "axs[3].set_xticklabels(feature_names, rotation=90)\n",
    "plt.show()\n",
    "axs[3].set_title('importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabNetEncoder(\n",
       "  (input_bn): BatchNorm1d(13, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "  (input_splitter): FeatureTransformer(\n",
       "    (shared_block): FeatureBlock(\n",
       "      (shared_layers): ModuleList(\n",
       "        (0): Linear(in_features=13, out_features=16, bias=False)\n",
       "      )\n",
       "      (glu_blocks): ModuleList(\n",
       "        (0): GLUBlock(\n",
       "          (fc): Linear(in_features=13, out_features=16, bias=False)\n",
       "          (gbn): GhostBatchNorm(\n",
       "            (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (indep_block): FeatureBlock(\n",
       "      (glu_blocks): ModuleList(\n",
       "        (0): GLUBlock(\n",
       "          (fc): Linear(in_features=8, out_features=16, bias=False)\n",
       "          (gbn): GhostBatchNorm(\n",
       "            (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feats_transformers): ModuleList(\n",
       "    (0): FeatureTransformer(\n",
       "      (shared_block): FeatureBlock(\n",
       "        (shared_layers): ModuleList(\n",
       "          (0): Linear(in_features=13, out_features=16, bias=False)\n",
       "        )\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=13, out_features=16, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (indep_block): FeatureBlock(\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=8, out_features=16, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): FeatureTransformer(\n",
       "      (shared_block): FeatureBlock(\n",
       "        (shared_layers): ModuleList(\n",
       "          (0): Linear(in_features=13, out_features=16, bias=False)\n",
       "        )\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=13, out_features=16, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (indep_block): FeatureBlock(\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=8, out_features=16, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): FeatureTransformer(\n",
       "      (shared_block): FeatureBlock(\n",
       "        (shared_layers): ModuleList(\n",
       "          (0): Linear(in_features=13, out_features=16, bias=False)\n",
       "        )\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=13, out_features=16, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (indep_block): FeatureBlock(\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=8, out_features=16, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (atten_transformers): ModuleList(\n",
       "    (0): AttentiveTransformer(\n",
       "      (fc): Linear(in_features=4, out_features=13, bias=False)\n",
       "      (gbn): GhostBatchNorm(\n",
       "        (bn): BatchNorm1d(13, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (selector): Sparsemax()\n",
       "    )\n",
       "    (1): AttentiveTransformer(\n",
       "      (fc): Linear(in_features=4, out_features=13, bias=False)\n",
       "      (gbn): GhostBatchNorm(\n",
       "        (bn): BatchNorm1d(13, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (selector): Sparsemax()\n",
       "    )\n",
       "    (2): AttentiveTransformer(\n",
       "      (fc): Linear(in_features=4, out_features=13, bias=False)\n",
       "      (gbn): GhostBatchNorm(\n",
       "        (bn): BatchNorm1d(13, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (selector): Sparsemax()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(tabnet._model, 'tabnet_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
