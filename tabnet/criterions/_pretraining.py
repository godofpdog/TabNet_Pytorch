""" Implementations of pre-training losses. """

import torch
from ._base import Loss
from ..mixin import PretrainingTaskMixin


class TabNetPretrainingLoss(Loss, PretrainingTaskMixin):
    """


    """
    def __init__(self, population_std=None, epsilon=1e-9):
        super(TabNetPretrainingLoss, self).__init__()
        """
        Initialization of `TabNetPretrainingLoss` module.

        Arguments:
            population_std (torch.Tensor):
                The population standard deviation of entire ground truth data.
                If None, use the batch statistics.

            epsilon (float):
                A small float value to avoid ZeroDivisionError

        Returns:
            None 

        """
        if population_std is not None and not isinstance(population_std, torch.Tensor):
            raise TypeError(
                    'Argument `population_std` must be a `torch.Tensor`, '
                    'but got `{}`'.format(type(population_std))
                )
        
        self.population_std = population_std
        self.epsilon = epsilon

    def score_func(self, preds, targets, mask):
        """
        Implementation of self-supervised pre-training loss function.

        Arguments:
            pred (torch.Tensor):
                Reconstruction from tabnet decoder.

            targets (torch.Tensor):
                The emnedded features from embedding encoder.

            mask (torch.Tensor):
                The binary mask generated by binary masker for the reconstruction task.

            * Note:
                - The true targets : `emnedded features` * `mask`.

        Returns:
            loss (torch.Tensor).
                The pre-training loss.

            * Note:
                - If use the batch std, take mean per batch.

        """
        errors = preds - targets
        reconstructed_errors = torch.mul(errors, mask) ** 2

        if self.population_std is not None:
            stds = self.population_std
        else:
            stds = torch.std(targets, dim=0) ** 2 + self.epsilon  # NOTE std ^ 2
        
        feats_loss = torch.matmul(reconstructed_errors, 1 / stds)  # sum over feats

        if self.population_std is not None:
            loss = torch.sum(feats_loss)
        else:
            num_reconstructions = torch.sum(mask, dim=1)
            loss = torch.mean(feats_loss / (num_reconstructions + self.epsilon))

        return loss


class SwapDAELoss(Loss, PretrainingTaskMixin):
    pass
