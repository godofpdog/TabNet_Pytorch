{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnet.estimator import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_classifier = TabNetClassifier(\n",
    "    input_dims=30, output_dims=[1], logger=None, is_cuda=False,\n",
    "    reprs_dims=8, atten_dims=4, num_steps=3, num_indep=1, num_shared=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabNetClassifier(atten_dims=4, batch_size=1024, cate_dims=None,\n",
       "                 cate_embed_dims=1, cate_indices=None, criterions=None,\n",
       "                 gamma=1.3, input_dims=30, is_cuda=False, is_shuffle=True,\n",
       "                 logger=None, mask_type='sparsemax', momentum=0.03, num_indep=1,\n",
       "                 num_shared=1, num_steps=3, num_workers=4, output_dims=[1],\n",
       "                 pin_memory=True, reprs_dims=8, task_weights=1,\n",
       "                 virtual_batch_size=128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabnet_classifier.build(path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingEncoder()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(tabnet_classifier._model, 'embedding_encoder', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabNetEncoder(\n",
       "  (input_bn): BatchNorm1d(30, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "  (input_splitter): FeatureTransformer(\n",
       "    (shared_block): FeatureBlock(\n",
       "      (shared_layers): ModuleList(\n",
       "        (0): Linear(in_features=30, out_features=24, bias=False)\n",
       "      )\n",
       "      (glu_blocks): ModuleList(\n",
       "        (0): GLUBlock(\n",
       "          (fc): Linear(in_features=30, out_features=24, bias=False)\n",
       "          (gbn): GhostBatchNorm(\n",
       "            (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (indep_block): FeatureBlock(\n",
       "      (glu_blocks): ModuleList(\n",
       "        (0): GLUBlock(\n",
       "          (fc): Linear(in_features=12, out_features=24, bias=False)\n",
       "          (gbn): GhostBatchNorm(\n",
       "            (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feats_transformers): ModuleList(\n",
       "    (0): FeatureTransformer(\n",
       "      (shared_block): FeatureBlock(\n",
       "        (shared_layers): ModuleList(\n",
       "          (0): Linear(in_features=30, out_features=24, bias=False)\n",
       "        )\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=30, out_features=24, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (indep_block): FeatureBlock(\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=12, out_features=24, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): FeatureTransformer(\n",
       "      (shared_block): FeatureBlock(\n",
       "        (shared_layers): ModuleList(\n",
       "          (0): Linear(in_features=30, out_features=24, bias=False)\n",
       "        )\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=30, out_features=24, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (indep_block): FeatureBlock(\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=12, out_features=24, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): FeatureTransformer(\n",
       "      (shared_block): FeatureBlock(\n",
       "        (shared_layers): ModuleList(\n",
       "          (0): Linear(in_features=30, out_features=24, bias=False)\n",
       "        )\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=30, out_features=24, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (indep_block): FeatureBlock(\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=12, out_features=24, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (atten_transformers): ModuleList(\n",
       "    (0): AttentiveTransformer(\n",
       "      (fc): Linear(in_features=4, out_features=30, bias=False)\n",
       "      (gbn): GhostBatchNorm(\n",
       "        (bn): BatchNorm1d(30, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (selector): Sparsemax()\n",
       "    )\n",
       "    (1): AttentiveTransformer(\n",
       "      (fc): Linear(in_features=4, out_features=30, bias=False)\n",
       "      (gbn): GhostBatchNorm(\n",
       "        (bn): BatchNorm1d(30, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (selector): Sparsemax()\n",
       "    )\n",
       "    (2): AttentiveTransformer(\n",
       "      (fc): Linear(in_features=4, out_features=30, bias=False)\n",
       "      (gbn): GhostBatchNorm(\n",
       "        (bn): BatchNorm1d(30, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (selector): Sparsemax()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(tabnet_classifier._model, 'tabnet_encoder', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnet.core import ModelConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = ModelConverter.to_pretrain(tabnet_classifier._model, 'xxx', 'xxx', tabnet_classifier._model_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PretrainModel(\n",
       "  (embedding_encoder): EmbeddingEncoder()\n",
       "  (tabnet_encoder): TabNetEncoder(\n",
       "    (input_bn): BatchNorm1d(30, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (input_splitter): FeatureTransformer(\n",
       "      (shared_block): FeatureBlock(\n",
       "        (shared_layers): ModuleList(\n",
       "          (0): Linear(in_features=30, out_features=24, bias=False)\n",
       "        )\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=30, out_features=24, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (indep_block): FeatureBlock(\n",
       "        (glu_blocks): ModuleList(\n",
       "          (0): GLUBlock(\n",
       "            (fc): Linear(in_features=12, out_features=24, bias=False)\n",
       "            (gbn): GhostBatchNorm(\n",
       "              (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feats_transformers): ModuleList(\n",
       "      (0): FeatureTransformer(\n",
       "        (shared_block): FeatureBlock(\n",
       "          (shared_layers): ModuleList(\n",
       "            (0): Linear(in_features=30, out_features=24, bias=False)\n",
       "          )\n",
       "          (glu_blocks): ModuleList(\n",
       "            (0): GLUBlock(\n",
       "              (fc): Linear(in_features=30, out_features=24, bias=False)\n",
       "              (gbn): GhostBatchNorm(\n",
       "                (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (indep_block): FeatureBlock(\n",
       "          (glu_blocks): ModuleList(\n",
       "            (0): GLUBlock(\n",
       "              (fc): Linear(in_features=12, out_features=24, bias=False)\n",
       "              (gbn): GhostBatchNorm(\n",
       "                (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): FeatureTransformer(\n",
       "        (shared_block): FeatureBlock(\n",
       "          (shared_layers): ModuleList(\n",
       "            (0): Linear(in_features=30, out_features=24, bias=False)\n",
       "          )\n",
       "          (glu_blocks): ModuleList(\n",
       "            (0): GLUBlock(\n",
       "              (fc): Linear(in_features=30, out_features=24, bias=False)\n",
       "              (gbn): GhostBatchNorm(\n",
       "                (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (indep_block): FeatureBlock(\n",
       "          (glu_blocks): ModuleList(\n",
       "            (0): GLUBlock(\n",
       "              (fc): Linear(in_features=12, out_features=24, bias=False)\n",
       "              (gbn): GhostBatchNorm(\n",
       "                (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): FeatureTransformer(\n",
       "        (shared_block): FeatureBlock(\n",
       "          (shared_layers): ModuleList(\n",
       "            (0): Linear(in_features=30, out_features=24, bias=False)\n",
       "          )\n",
       "          (glu_blocks): ModuleList(\n",
       "            (0): GLUBlock(\n",
       "              (fc): Linear(in_features=30, out_features=24, bias=False)\n",
       "              (gbn): GhostBatchNorm(\n",
       "                (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (indep_block): FeatureBlock(\n",
       "          (glu_blocks): ModuleList(\n",
       "            (0): GLUBlock(\n",
       "              (fc): Linear(in_features=12, out_features=24, bias=False)\n",
       "              (gbn): GhostBatchNorm(\n",
       "                (bn): BatchNorm1d(24, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (atten_transformers): ModuleList(\n",
       "      (0): AttentiveTransformer(\n",
       "        (fc): Linear(in_features=4, out_features=30, bias=False)\n",
       "        (gbn): GhostBatchNorm(\n",
       "          (bn): BatchNorm1d(30, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (selector): Sparsemax()\n",
       "      )\n",
       "      (1): AttentiveTransformer(\n",
       "        (fc): Linear(in_features=4, out_features=30, bias=False)\n",
       "        (gbn): GhostBatchNorm(\n",
       "          (bn): BatchNorm1d(30, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (selector): Sparsemax()\n",
       "      )\n",
       "      (2): AttentiveTransformer(\n",
       "        (fc): Linear(in_features=4, out_features=30, bias=False)\n",
       "        (gbn): GhostBatchNorm(\n",
       "          (bn): BatchNorm1d(30, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (selector): Sparsemax()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pretext_model): TabNetPretextModel(\n",
       "    (masker): BinaryMasker()\n",
       "    (decoder): TabNetDecoder(\n",
       "      (feats_transformers): ModuleList(\n",
       "        (0): FeatureTransformer(\n",
       "          (shared_block): FeatureBlock(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=8, out_features=16, bias=False)\n",
       "            )\n",
       "            (glu_blocks): ModuleList(\n",
       "              (0): GLUBlock(\n",
       "                (fc): Linear(in_features=8, out_features=16, bias=False)\n",
       "                (gbn): GhostBatchNorm(\n",
       "                  (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (indep_block): FeatureBlock(\n",
       "            (glu_blocks): ModuleList(\n",
       "              (0): GLUBlock(\n",
       "                (fc): Linear(in_features=8, out_features=16, bias=False)\n",
       "                (gbn): GhostBatchNorm(\n",
       "                  (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): FeatureTransformer(\n",
       "          (shared_block): FeatureBlock(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=8, out_features=16, bias=False)\n",
       "            )\n",
       "            (glu_blocks): ModuleList(\n",
       "              (0): GLUBlock(\n",
       "                (fc): Linear(in_features=8, out_features=16, bias=False)\n",
       "                (gbn): GhostBatchNorm(\n",
       "                  (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (indep_block): FeatureBlock(\n",
       "            (glu_blocks): ModuleList(\n",
       "              (0): GLUBlock(\n",
       "                (fc): Linear(in_features=8, out_features=16, bias=False)\n",
       "                (gbn): GhostBatchNorm(\n",
       "                  (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): FeatureTransformer(\n",
       "          (shared_block): FeatureBlock(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=8, out_features=16, bias=False)\n",
       "            )\n",
       "            (glu_blocks): ModuleList(\n",
       "              (0): GLUBlock(\n",
       "                (fc): Linear(in_features=8, out_features=16, bias=False)\n",
       "                (gbn): GhostBatchNorm(\n",
       "                  (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (indep_block): FeatureBlock(\n",
       "            (glu_blocks): ModuleList(\n",
       "              (0): GLUBlock(\n",
       "                (fc): Linear(in_features=8, out_features=16, bias=False)\n",
       "                (gbn): GhostBatchNorm(\n",
       "                  (bn): BatchNorm1d(16, eps=1e-05, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fc_layers): ModuleList(\n",
       "        (0): Linear(in_features=8, out_features=30, bias=False)\n",
       "        (1): Linear(in_features=8, out_features=30, bias=False)\n",
       "        (2): Linear(in_features=8, out_features=30, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn((256, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0712,  0.2836,  0.4401,  ...,  0.3297,  0.3548,  0.1276],\n",
       "         [-0.0996, -0.0576, -0.5252,  ..., -0.3660,  0.3237,  0.0409],\n",
       "         [ 0.1263, -0.2210, -0.3481,  ...,  0.0687, -0.1204, -0.2660],\n",
       "         ...,\n",
       "         [ 0.3916,  0.0771, -0.4144,  ..., -0.4280,  0.6369, -0.2283],\n",
       "         [ 0.3625, -0.0640, -0.2125,  ...,  0.3270, -0.9452, -0.2594],\n",
       "         [ 0.0695,  0.1884,  0.0155,  ...,  0.2076,  0.1412, -0.0795]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor(-1.2477, grad_fn=<DivBackward0>),\n",
       " tensor([[0., 1., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 1.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnet.core import get_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
